{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLIERS DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Select numerical columns for analysis\n",
    "numerical_cols = ['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Age', 'Education', 'Income']\n",
    "\n",
    "# Create boxplots for numerical variables\n",
    "sns.boxplot(data=data[numerical_cols] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    z_scores = [(x - mean) / std_dev for x in data]\n",
    "    outlier_indices = np.where(np.abs(z_scores) > threshold)[0]\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "\n",
    "    # Example usage:\n",
    "    data = df[f'{col}'].to_list()\n",
    "    outliers_indices = detect_outliers_zscore(data)\n",
    "    df[f'{col}'] = df[f'{col}'].drop(outliers_indices)\n",
    "    print(\"Indices of outliers:\", outliers_indices , len(outliers_indices) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the min and max values of every column\n",
    "# column_min_max = df.agg(['min', 'max'])\n",
    "\n",
    "# column_min_max.to_csv('temo.csv' , index= False)\n",
    "\n",
    "# print(\"Min and Max values of every column:\")\n",
    "# print(column_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Load your dataset\n",
    "data = df\n",
    "\n",
    "# Separate features and target variable\n",
    "x = data.drop('Diabetes_binary', axis=1)\n",
    "y = data['Diabetes_binary']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure target variable contains non-negative values\n",
    "# If it's a classification problem with binary classes, this step may not be necessary\n",
    "if y.min() < 0:\n",
    "    raise ValueError(\"Target variable contains negative values. Preprocessing required.\")\n",
    "\n",
    "\n",
    "# Feature Scaling with Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform SelectKBest feature selection using chi2 (for classification)\n",
    "k_best = SelectKBest(score_func=chi2, k=5)\n",
    "X_train_kbest = k_best.fit_transform(X_train_scaled, y_train)\n",
    "X_test_kbest = k_best.transform(X_test_scaled)\n",
    "\n",
    "# Print the selected feature indices\n",
    "print(\"Selected feature indices:\", k_best.get_support(indices=True))\n",
    "\n",
    "# Train a model with the selected features (e.g., Random Forest)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_kbest, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = model.score(X_test_kbest, y_test)\n",
    "print(\"Accuracy with selected features:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISULIZATION AND DATA SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot=True,fmt='0.1f',linewidth=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Diabetes_binary',data=df,palette=['g','r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('Distribution plot')\n",
    "sns.distplot(df[\"Diabetes_binary\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()\n",
    "df.corr().to_csv('correlation.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHYSICAL ACTIVITY ON OUTPUT\n",
    "\n",
    "count_phys =0\n",
    "count_nonphy=0\n",
    "\n",
    "count_1 =0\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "row_count = df.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_ones = df[(df['Diabetes_binary'] == 0) & (df['PhysActivity'] == 0)].shape[0]\n",
    "count_nonphy = count_phys + count_ones\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 0) & (df['PhysActivity'] == 1)].shape[0]\n",
    "count_phys = count_phys + count_ones\n",
    "print(count_ones/row_count)\n",
    "\n",
    "\n",
    "count_ones = df[(df['Diabetes_binary'] == 1) & (df['PhysActivity'] == 1)].shape[0]\n",
    "count_phys = count_phys + count_ones\n",
    "print(count_ones/row_count)\n",
    "\n",
    "\n",
    "print( \"physics with diabates\" , count_ones/count_phys   )\n",
    "\n",
    "\n",
    "count_ones = df[(df['Diabetes_binary'] == 1) & (df['PhysActivity'] == 0)].shape[0]\n",
    "count_nonphy = count_phys + count_ones\n",
    "print(count_ones/row_count)\n",
    "\n",
    "print( \"non-physics with diabates\" ,  count_ones/count_nonphy  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHYSICAL ACTIVITY ON OUTPUT\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "row_count = df.shape[0]\n",
    "\n",
    "count_ones = df[(df['Diabetes_binary'] == 1) & (df['GenHlth'] == 1)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 1) & (df['GenHlth'] == 2)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 1) & (df['GenHlth'] == 3)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 1) & (df['GenHlth'] == 4)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 1) & (df['GenHlth'] == 5)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "\n",
    "print(\".....\")\n",
    "\n",
    "count_ones = df[(df['Diabetes_binary'] == 0) & (df['GenHlth'] == 1)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 0) & (df['GenHlth'] == 2)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 0) & (df['GenHlth'] == 3)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 0) & (df['GenHlth'] == 4)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "count_ones = df[(df['Diabetes_binary'] == 0) & (df['GenHlth'] == 5)].shape[0]\n",
    "print(count_ones/row_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGESTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('Diabetes_binary' , axis=1)\n",
    "y = df['Diabetes_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting of data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features (optional but recommended for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Creating a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Training the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Getting additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Training the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Getting additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a Gradient Boosting Machine classifier\n",
    "gbm_classifier = xgb.XGBClassifier(random_state=42  )\n",
    "\n",
    "# Training the classifier on the training data\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "y_pred = gbm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Getting additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the trained model to a file\n",
    "filename = 'xgb_trained_model.model'\n",
    "gbm_classifier.save_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGESTIC REGRESSION ( SELECTED ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[ ['HighBP' , 'HighChol', 'BMI' ,'GenHlth' , 'DiffWalk' , 'HeartDiseaseorAttack' , 'PhysHlth' , 'Age' , 'AnyHealthcare' ]  ]\n",
    "y = df['Diabetes_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting of data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features (optional but recommended for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Creating a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Training the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Getting additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST (SELECTED ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a Gradient Boosting Machine classifier\n",
    "gbm_classifier = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Training the classifier on the training data\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "y_pred = gbm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Getting additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST WITH FEATURE ENGENEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, [ 1 ,13 ,15, 16]] \n",
    "y = df.iloc[:, [0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a Gradient Boosting Machine classifier\n",
    "gbm_classifier = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Training the classifier on the training data\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "y_pred = gbm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Getting additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
